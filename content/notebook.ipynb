{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbe5e4f",
   "metadata": {},
   "source": [
    "# Erklärbarkeit von KI-Modellen\n",
    "\n",
    "Mit Künstlicher Intelligenz (KI) beschäftigen sich Informatikerinnen und\n",
    "Informatiker bereits seit 1955. Die Mehrheit der Bevölkerung in Deutschland hat\n",
    "jedoch erst mit der Veröffentlichung von ChatGPT im November 2022 erstmals\n",
    "bewusst ein KI-System genutzt. Seitdem findet eine überfällige Diskussion statt,\n",
    "wie wir als Gesellschaft zukünftig mit KI-Anwendungen umgehen sollen. Eine\n",
    "besondere Herausforderung in diesem Kontext ist die **Erklärbarkeit von\n",
    "KI-Modellen**, also die Nachvollziehbarkeit, wie KI-Systeme ihre Prognosen und\n",
    "Entscheidungen treffen. Die Herausforderung besteht besonders bei komplexen\n",
    "Modellen wie tiefen neuronalen Netzen, deren Struktur und Entscheidungslogik für\n",
    "Menschen oft schwer nachvollziehbar sind.\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "```{admonition} Lernziele\n",
    ":class: goals\n",
    "* Sie verstehen, warum **Erklärbarkeit von KI-Modellen** gesellschaftlich\n",
    "  relevant ist.\n",
    "* Sie kennen die Begriffe **White-Box-Modell** und **Black-Box-Modell**.\n",
    "* Sie können die Funktionsweise des Erklärungsmodells **LIME** (Local\n",
    "  Interpretable Model-Agnostic Explanations) beschreiben und an einem einfachen\n",
    "  Beispiel anwenden.\n",
    "* Sie können Erklärungswerkzeuge unterscheiden nach\n",
    "  1. **Komplexität** (intrinsische Modelle oder Post-hoc-Methoden),\n",
    "  2. **Umfang** (global oder lokal) und\n",
    "  3. **Modellabhängigkeit** (modellspezifisch oder modellagnostisch).\n",
    "```\n",
    "\n",
    "## Warum brauchen wir erklärbare KI-Modelle?\n",
    "\n",
    "Neuronale Netze, insbesondere mehrschichtige neuronale Netze im Deep Learning,\n",
    "haben in den letzten Jahren zu einem enormen Anstieg bei der Anwendung von\n",
    "KI-Systemen geführt. Laut einer [Studie von\n",
    "NextMSC](https://www.nextmsc.com/report/artificial-intelligence-market) wird\n",
    "sich der weltweite Umsatz von KI (einschließlich Anwendungen, Infrastruktur\n",
    "sowie IT- und Unternehmensdienstleistungen) bis zum Jahr 2030 verzwanzigfachen.\n",
    "Dennoch zögern deutsche Unternehmen noch, KI-Systeme umfassend einzusetzen, wie\n",
    "der Bericht [Statista → KI Perspektive der deutschen\n",
    "Wirtschaft](https://de.statista.com/themen/9400/ki-in-der-deutschen-wirtschaft/#topicOverview)\n",
    "zeigt. Viele Unternehmen zweifeln an der Qualität der verfügbaren Daten für das\n",
    "Training der KI-Systeme und stufen die mangelnde Transparenz der\n",
    "KI-Entscheidungen als Risiko ein. Ohne Vertrauen in die Vorhersagefähigkeiten\n",
    "der KI bleiben sie daher vorsichtig bei der Einführung solcher Systeme.\n",
    "\n",
    "Selbst für Fachleute bleiben manche KI-Modelle wie beispielsweise die tiefen\n",
    "neuronalen Netze intransparent. Hier setzen erklärbare KI-Modelle an: Sie sollen\n",
    "das Vertrauen in die Entscheidungen der Systeme stärken und nachvollziehbare\n",
    "Einblicke in die Entscheidungslogik der KI ermöglichen.\n",
    "\n",
    "## Anwendungsbeispiel: das Schuheinlagen-Orakel\n",
    "\n",
    "Anhand eines fiktiven Anwendungsbeispiels aus der Produktion erarbeiten wir uns\n",
    "das Thema Erklärbarkeit von KI-Modellen. Angenommen, eine Firma stellt\n",
    "personalisierte Schuheinlagen her. Dabei sollen die Einlagen sowohl an die\n",
    "Geometrie des Fußes angepasst werden als auch an die Druckbelastungen des Fußes\n",
    "beim Gehen. Sowohl Geometrie als auch Druckbelastungen werden digital erfasst,\n",
    "und basierend auf den Messungen wird die Schuheinlage in einzelne\n",
    "Belastungszonen eingeteilt und in 3D gedruckt. Zwar ist die Firma in diesem\n",
    "Beispiel fiktiv, ein Prototyp dieses Prozesses existiert jedoch bereits (siehe\n",
    "{cite}`voelz:2023`) und könnte so tatsächlich in naher Zukunft von einem\n",
    "Start-Up umgesetzt werden.\n",
    "\n",
    "Beim 3D-Druck gibt es Zielkonflikte. Einerseits soll möglichst wenig Material\n",
    "eingesetzt werden, andererseits muss die gedruckte Struktur dennoch die\n",
    "Belastung aushalten, die für diese Belastungszone vorgesehen ist. Zur\n",
    "Unterstützung der Entwicklung der personalisierten Schuheinlage gibt es ein\n",
    "KI-Modell, das die maximale Kraft (in Newton) prognostiziert, mit der eine\n",
    "3D-gedruckte Gitterstruktur belastet werden kann. Aus diesen 3D-Bauteilen werden\n",
    "dann die Schuheinlagen zusammengesetzt.\n",
    "\n",
    "Spaßeshalber nennen wir dieses KI-Modell **Schuheinlagen-Orakel**, denn leider\n",
    "liegt es nur binär vor. Daher müssen wir auch das Modul `dill` benutzen, um es\n",
    "zu laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a75ffc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=8, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">?<span>Documentation for RandomForestRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestRegressor(max_depth=8, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=8, random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "data = pd.read_csv('data/messungen.csv')\n",
    "\n",
    "\n",
    "\n",
    "ki_modell = RandomForestRegressor(max_depth=8, random_state=42, min_samples_leaf=1)\n",
    "ki_modell.fit(data[['Zellenform', 'Zellengroesse', 'Fuellgrad']], data['Maximale Kraft'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a9f3f6",
   "metadata": {},
   "source": [
    "Als nächstes benutzen wir die eingebaute Hilfe des KI-Systems, um mehr über das\n",
    "Schuheinlagen-Orakel zu erfahren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e40de91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RandomForestRegressor in module sklearn.ensemble._forest object:\n",
      "\n",
      "class RandomForestRegressor(ForestRegressor)\n",
      " |  RandomForestRegressor(n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
      " |\n",
      " |  A random forest regressor.\n",
      " |\n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  regressors on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  Trees in the forest use the best split strategy, i.e. equivalent to passing\n",
      " |  `splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |\n",
      " |  For a comparison between tree-based ensemble models see the example\n",
      " |  :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n",
      " |\n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |\n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |\n",
      " |  criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"},             default=\"squared_error\"\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"squared_error\" for the mean squared error, which is equal to\n",
      " |      variance reduction as feature selection criterion and minimizes the L2\n",
      " |      loss using the mean of each terminal node, \"friedman_mse\", which uses\n",
      " |      mean squared error with Friedman's improvement score for potential\n",
      " |      splits, \"absolute_error\" for the mean absolute error, which minimizes\n",
      " |      the L1 loss using the median of each terminal node, and \"poisson\" which\n",
      " |      uses reduction in Poisson deviance to find splits.\n",
      " |      Training using \"absolute_error\" is significantly slower\n",
      " |      than when using \"squared_error\".\n",
      " |\n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |\n",
      " |      .. versionadded:: 1.0\n",
      " |         Poisson criterion.\n",
      " |\n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |\n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |\n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |\n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |\n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |\n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |\n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |\n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |\n",
      " |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |\n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `max(1, int(max_features * n_features_in_))` features are considered at each\n",
      " |        split.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None or 1.0, then `max_features=n_features`.\n",
      " |\n",
      " |      .. note::\n",
      " |          The default of 1.0 is equivalent to bagged trees and more\n",
      " |          randomness can be achieved by setting smaller values, e.g. 0.3.\n",
      " |\n",
      " |      .. versionchanged:: 1.1\n",
      " |          The default of `max_features` changed from `\"auto\"` to 1.0.\n",
      " |\n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |\n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |\n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |\n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |\n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |\n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |\n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |\n",
      " |      .. versionadded:: 0.19\n",
      " |\n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |\n",
      " |  oob_score : bool or callable, default=False\n",
      " |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      " |      By default, :func:`~sklearn.metrics.r2_score` is used.\n",
      " |      Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
      " |      custom metric. Only available if `bootstrap=True`.\n",
      " |\n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |\n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |\n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |\n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`Glossary <warm_start>` and\n",
      " |      :ref:`tree_ensemble_warm_start` for details.\n",
      " |\n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |\n",
      " |      .. versionadded:: 0.22\n",
      " |\n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |\n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      " |\n",
      " |      .. versionadded:: 0.22\n",
      " |\n",
      " |  monotonic_cst : array-like of int of shape (n_features), default=None\n",
      " |      Indicates the monotonicity constraint to enforce on each feature.\n",
      " |        - 1: monotonically increasing\n",
      " |        - 0: no constraint\n",
      " |        - -1: monotonically decreasing\n",
      " |\n",
      " |      If monotonic_cst is None, no constraints are applied.\n",
      " |\n",
      " |      Monotonicity constraints are not supported for:\n",
      " |        - multioutput regressions (i.e. when `n_outputs_ > 1`),\n",
      " |        - regressions trained on data with missing values.\n",
      " |\n",
      " |      Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
      " |\n",
      " |      .. versionadded:: 1.4\n",
      " |\n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |\n",
      " |      .. versionadded:: 1.2\n",
      " |         `base_estimator_` was renamed to `estimator_`.\n",
      " |\n",
      " |  estimators_ : list of DecisionTreeRegressor\n",
      " |      The collection of fitted sub-estimators.\n",
      " |\n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |\n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |\n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |\n",
      " |      .. versionadded:: 0.24\n",
      " |\n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |\n",
      " |      .. versionadded:: 1.0\n",
      " |\n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |\n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |\n",
      " |  oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |      Prediction computed with out-of-bag estimate on the training set.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |\n",
      " |  estimators_samples_ : list of arrays\n",
      " |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      " |      estimator. Each subset is defined by an array of the indices selected.\n",
      " |\n",
      " |      .. versionadded:: 1.4\n",
      " |\n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
      " |  sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n",
      " |      tree regressors.\n",
      " |  sklearn.ensemble.HistGradientBoostingRegressor : A Histogram-based Gradient\n",
      " |      Boosting Regression Tree, very fast for big datasets (n_samples >=\n",
      " |      10_000).\n",
      " |\n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |\n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |\n",
      " |  The default value ``max_features=1.0`` uses ``n_features``\n",
      " |  rather than ``n_features / 3``. The latter was originally suggested in\n",
      " |  [1], whereas the former was more recently justified empirically in [2].\n",
      " |\n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |\n",
      " |  .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      " |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      " |  ...                        random_state=0, shuffle=False)\n",
      " |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      " |  >>> regr.fit(X, y)\n",
      " |  RandomForestRegressor(...)\n",
      " |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      " |  [-8.32987858]\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      RandomForestRegressor\n",
      " |      ForestRegressor\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  set_fit_request(self: sklearn.ensemble._forest.RandomForestRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.RandomForestRegressor\n",
      " |      Request metadata passed to the ``fit`` method.\n",
      " |\n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      The options for each parameter are:\n",
      " |\n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |\n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |\n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |\n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |\n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |\n",
      " |      .. versionadded:: 1.3\n",
      " |\n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |\n",
      " |  set_score_request(self: sklearn.ensemble._forest.RandomForestRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.RandomForestRegressor\n",
      " |      Request metadata passed to the ``score`` method.\n",
      " |\n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      The options for each parameter are:\n",
      " |\n",
      " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      " |\n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      " |\n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |\n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |\n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |\n",
      " |      .. versionadded:: 1.3\n",
      " |\n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestRegressor:\n",
      " |\n",
      " |  predict(self, X)\n",
      " |      Predict regression target for X.\n",
      " |\n",
      " |      The predicted regression target of an input sample is computed as the\n",
      " |      mean predicted regression targets of the trees in the forest.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted values.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |\n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |\n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |\n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |\n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |\n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |\n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |\n",
      " |      .. versionadded:: 0.18\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |\n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |\n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |\n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |\n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |\n",
      " |  estimators_samples_\n",
      " |      The subset of drawn samples for each base estimator.\n",
      " |\n",
      " |      Returns a dynamically generated list of indices identifying\n",
      " |      the samples used for fitting each member of the ensemble, i.e.,\n",
      " |      the in-bag samples.\n",
      " |\n",
      " |      Note: the list is re-created at each call to the property in order\n",
      " |      to reduce the object memory footprint by not storing the sampling\n",
      " |      data. Thus fetching the property may be slower than expected.\n",
      " |\n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |\n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |\n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |\n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |\n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __setstate__(self, state)\n",
      " |\n",
      " |  __sklearn_clone__(self)\n",
      " |\n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |\n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |\n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |\n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  __init_subclass__(**kwargs) from abc.ABCMeta\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |\n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |\n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |\n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ki_modell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde6187",
   "metadata": {},
   "source": [
    "Die Hilfe gibt Auskunft darüber, wie das Schuheinlagen-Orakel verwendet werden\n",
    "kann. Das KI-Modell liefert Prognosen zur maximalen Kraft, die ein Bauteil\n",
    "aushalten kann, basierend auf den Eingabemerkmalen Zellenform, Zellengröße und\n",
    "Füllgrad. Die Messung der maximalen Kraft wird dabei durch die Be- und\n",
    "Entlastung des Bauteils bestimmt.\n",
    "\n",
    "```{figure} ./pics/messung.mp4\n",
    ":alt: Video der Be- und Entlastungsmessung\n",
    ":align: center\n",
    ":name: messung\n",
    ":width: 100%\n",
    "Video der Be- und Entlastungsmessung eines Bauteils\n",
    "(Quelle: Tim Schwitzner {cite}`schwitzner:2024`).\n",
    "```\n",
    "\n",
    "Laut Dokumentation ist die Zellenform entweder `1` für eine X-Zelle\n",
    "oder `2` für einen Gyroiden. Was sich hinter diesen Fachbegriffen verbirgt,\n",
    "können nur die Ingenieure beantworten, die diesen Produktionsprozess entwickelt\n",
    "haben (siehe Abbildung 1).\n",
    "\n",
    "```{figure} pics/zellenform.png\n",
    ":alt: Zellenform der Bauteile\n",
    ":align: center\n",
    ":name: zellenform\n",
    "Zellenform der Bauteile: links X-Zelle codiert als 1 und rechts Gyroid codiert als 2  \n",
    "(Quelle: Tim Schwitzner {cite}`schwitzner:2024`).\n",
    "```\n",
    "\n",
    "Darüber hinaus muss die Zellengröße zwischen $\\pu{2 mm}$ und $\\pu{10 mm}$\n",
    "liegen. Der Füllgrad liegt zwischen $\\pu{20 \\%}$ und $\\pu{45 \\%}$, wobei dieser\n",
    "Prozentwert als Fließkommazahl (Float) im Intervall $[0.2,0.45]$ angegeben wird.\n",
    "\n",
    "Um mit dem KI-Modell vertraut zu werden, lassen wir eine Prognose erstellen. Wir\n",
    "nutzen das Modul Pandas zur Verwaltung der Daten und importieren es daher in\n",
    "einem ersten Schritt mit seiner üblichen Abkürzung `pd`. Als nächstes definieren\n",
    "wir ein Bauteil. Dazu verwenden wir einen Pandas-DataFrame als Datenstruktur,\n",
    "der durch ein Dictionary initialisiert wird. Die Schlüssel des Dictionaries sind\n",
    "die Merkmale `'Zellenform'`, `'Zellengroesse'` und `'Fuellgrad'`. Die Werte sind\n",
    "Listen mit den entsprechenden Eigenschaften der 3D-gedruckten Bauteile. Auch\n",
    "wenn nur ein Bauteil betrachtet wird, ist eine Liste aufgrund der Syntax\n",
    "erforderlich. Zuletzt lassen wir uns die initialisierte Datenstruktur mit der\n",
    "Pandas-Methode `.head()` anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bfa72d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zellenform</th>\n",
       "      <th>Zellengroesse</th>\n",
       "      <th>Fuellgrad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Zellenform  Zellengroesse  Fuellgrad\n",
       "0           1            3.0        0.3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "bauteil = pd.DataFrame({\n",
    "    'Zellenform': [1],\n",
    "    'Zellengroesse': [3.0],\n",
    "    'Fuellgrad': [0.3]\n",
    "})\n",
    "\n",
    "bauteil.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8521ad",
   "metadata": {},
   "source": [
    "Nun können wir die `predict()`-Methode nutzen, um die maximale Kraft\n",
    "prognostizieren zu lassen. Wir speichern das Ergebnis in der Variablen\n",
    "`maximale_kraft`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a413a6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42.335805]\n"
     ]
    }
   ],
   "source": [
    "maximale_kraft = ki_modell.predict(bauteil)\n",
    "print(maximale_kraft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a1027",
   "metadata": {},
   "source": [
    "Das Bauteil hält eine maximale Kraft von $\\pu{42.3 N}$ aus.\n",
    "\n",
    "Obwohl das Schuheinlagen-Orakel eine Prognose liefert, bleibt das System für uns\n",
    "eine **Black Box**. Die innere Struktur des KI-Modells ist nicht transparent;\n",
    "wir wissen nicht, *wie* das KI-Modell zu seiner Prognose gekommen ist.\n",
    "\n",
    "Ist ein solches Szenario realistisch? Tatsächlich kommen solche Szenarien\n",
    "häufiger vor als erwünscht. Verlassen beispielsweise Wissensträger ein\n",
    "Unternehmen, ist es oft nicht möglich, ihre Expertise ausreichend für die\n",
    "nachfolgenden Mitarbeiterinnen und Mitarbeiter zu dokumentieren. Aber auch die\n",
    "zunehmend *komplexeren KI-Modelle* sind für uns Menschen undurchsichtig. Auch\n",
    "können die Eingabedaten für ein KI-Modell so stark weiterverarbeitet worden\n",
    "sein, dass auch die Eingabedaten selbst nicht mehr für uns Menschen\n",
    "nachvollziehbar sind. Ist ein KI-Modell undurchsichtig oder sind die\n",
    "Eingangsdaten nicht nachvollziehbar, wird das KI-Modell als Black-Box-Modell\n",
    "bezeichnet.\n",
    "\n",
    "```{admonition} Was ist ... ein Black-Box-Modell?\n",
    ":class: note\n",
    "Ein KI-Modell wird als Black-Box-Modell bezeichnet, wenn es keine transparente\n",
    "Entscheidungslogik besitzt oder die Eingangsdaten nicht nachvollziehbar sind.\n",
    "```\n",
    "\n",
    "Das Gegenteil eines Black-Box-Modells ist das sogenannte **White-Box-Modell**.\n",
    "Es ist im Kontext der erklärbaren KI folgendermaßen definiert.\n",
    "\n",
    "```{admonition} Was ist ... ein White-Box-Modell?\n",
    ":class: note\n",
    "Ein KI-Modell wird als White-Box-Modell bezeichnet, wenn seine\n",
    "Entscheidungslogik transparent ist und es nachvollziehbare Eingangsdaten\n",
    "besitzt.\n",
    "```\n",
    "\n",
    "## Das LIME-Modell\n",
    "\n",
    "Kehren wir zurück zu dem fiktiven Anwendungsbeispiel und stellen uns vor, die\n",
    "Entwickler des Schuheinlagen-Orakels haben das Unternehmen verlassen. Die\n",
    "Konstruktionsabteilung möchte nun verstehen, warum für ein 3D-gedrucktes Bauteil\n",
    "mit der Zellenform X-Zelle, einer Zellengröße von $\\pu{3 mm}$ und einem Füllgrad\n",
    "von $\\pu{30 \\%}$ eine maximale Kraft von $\\pu{42.3 N}$ prognostiziert wird. Um\n",
    "ein KI-Modell erklärbar zu machen, gibt es verschiedene Ansätze. Ein häufig\n",
    "verwendetes Verfahren ist **LIME**. LIME ist ein Akronym und steht für\n",
    "\n",
    "**L**ocal **I**nterpretable **M**odel-agnostic **E**xplanations.\n",
    "\n",
    "*Lokal* bedeutet, dass eine Erklärung für ein einzelnes Beispiel gesucht wird.\n",
    "Gleichzeitig soll das LIME-Modell *interpretierbar* sein, also ein\n",
    "White-Box-Modell darstellen. *Modellagnostisch* bedeutet, dass die LIME-Methode\n",
    "unabhängig von der Struktur des zugrunde liegenden KI-Modells funktioniert und\n",
    "für verschiedene KI-Modelle eingesetzt werden kann.\n",
    "\n",
    "```{admonition} Wie funktioniert die LIME-Methode?\n",
    ":class: notes\n",
    "1. *Variation der Daten*: Für ein ausgewähltes Beispiel, die sogenannte\n",
    "   Referenz, erzeugen wir abgewandelte Varianten der Eingabedaten mit kleinen\n",
    "   Änderungen im Vergleich zum Referenzbeispiel.\n",
    "2. *Berechnung der Prognosen*: Für jede dieser leicht abgeänderten Eingabedaten\n",
    "   berechnen wir mit dem ursprünglichen KI-Modell eine Prognose.\n",
    "3. *Gewichtung der Eingabedaten*: Die abgeänderten Eingabedaten werden\n",
    "   gewichtet. Je ähnlicher eine Datenpunkt zur Referenz ist, desto höher ist das\n",
    "   Gewicht.\n",
    "4. *Training eines Ersatzmodells*: Wir trainieren ein einfaches, gut\n",
    "   interpretierbares Ersatzmodell (z.B. ein lineares Regressionsmodell oder einen\n",
    "   Entscheidungsbaum) auf den gewichteten, leicht abgeänderten Eingabedaten. Die\n",
    "   Prognosen des ursprünglichen Modells sind dabei die Ausgabedaten.\n",
    "5. *Erklärung der Prognose*: Da das Ersatzmodell aus Schritt 4 ein\n",
    "   White-Box-Modell ist, können wir es nun benutzen, um das Black-Box-KI-Modell\n",
    "   lokal zu erklären.\n",
    "```\n",
    "\n",
    "Obwohl für das LIME-Modell, das erstmals 2016 vorgestellt wurde\n",
    "{cite}`ribeiro:2016`, ein Python-Modul namens\n",
    "[lime](https://github.com/marcotcr/lime) existiert, erstellen wir das\n",
    "LIME-Modell hier Schritt für Schritt von Grund auf, um die Funktionsweise besser\n",
    "zu verstehen.\n",
    "\n",
    "### Schritt 1: Variation der Daten\n",
    "\n",
    "Als erstes ändern wir die Merkmale des ausgewählten Referenzbeispiels leicht ab.\n",
    "Da es nur zwei mögliche Zellenformen gibt, können wir diese nicht \"leicht\"\n",
    "variieren. Daher belassen wir es bei der Zellenform `X-Zelle` und generieren\n",
    "eine Liste mit `N` Einsen. Etwas einfacher wird es, wenn wir dazu die Funktion\n",
    "`np.ones()` des Moduls `NumPy` nutzen, das wir mit der üblichen Abkürzung `np`\n",
    "importieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f1bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 100\n",
    "variation_zellenform = np.ones(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bde6fd3",
   "metadata": {},
   "source": [
    "Anschließend möchten wir die Zellengröße und den Füllgrad leicht variieren.\n",
    "Dafür verwenden wir Zufallszahlen. Aus didaktischen Gründen fixieren wir den\n",
    "Seed der Zufallszahlen auf 42 mit `np.random.seed(42)`. Dann ziehen wir mit der\n",
    "Funktion `np.random.normal()` normalverteilte Zufallszahlen (mit Mittelwert 0\n",
    "und Standardabweichung 0.1) und addieren diese Zufallszahlen zur Zellengröße 3\n",
    "und zum Füllgrad 0.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) \n",
    "\n",
    "variation_zellengroesse = 3.0 + np.random.normal(0, 0.5, N)\n",
    "variation_fuellgrad = 0.3 + np.random.normal(0, 0.05, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efb604",
   "metadata": {},
   "source": [
    "Wir überprüfen visuell mit einem Streudiagramm (Scatterplot), ob die variierten\n",
    "Eingabedaten im zulässigen Bereich liegen. Die Zellengröße muss ja zwischen 2\n",
    "und 10 mm liegen, der Füllgrad im Intervall [0.2,0.45]. Dazu importieren wir das\n",
    "Modul `Plotly Express` als `px`. Der Scatterplot wird durch die Funktion\n",
    "`scatter()` erzeugt. Auf der x-Achse tragen wir mit `x=variation_zellengroesse`\n",
    "die Zellengrößen ein und auf der y-Achse mit `y=variation_fuellgrad` die\n",
    "Füllgrade der variierten Bauteile. Zusätzlich setzen wir mit dem optionalen\n",
    "Argument `title=` noch einen Titel. Zuletzt ergänzen wir noch das\n",
    "Referenzbeispiel durch einen zweiten Scatterplot mit `add_scatter()`. Insgesamt\n",
    "fixieren wir den Ausschnitt für die x-Achse auf [1.5, 4.5] und für die y-Achse\n",
    "auf [0.15, 0.45] mit `fig.update_layout(xaxis_range=[1.5, 4.5],\n",
    "yaxis_range=[0.15,0.45])`, damit das Referenzbeispiel im Zentrum des Diagramms\n",
    "liegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca46a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px \n",
    "\n",
    "fig = px.scatter(x=variation_zellengroesse, y=variation_fuellgrad,\n",
    "    title='Variierte Eingabedaten um ausgewähltes Referenzbeispiel (3, 0.3)'\n",
    ")\n",
    "fig.add_scatter(x=[3.0], y=[0.3], name='Referenz')\n",
    "fig.update_layout(xaxis_range=[1.5, 4.5], yaxis_range=[0.15,0.45])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e21667",
   "metadata": {},
   "source": [
    "Auf diese Weise erhalten wir die variierten Eingabedaten, die wir anschließend\n",
    "in einem Pandas-DataFrame zusammenfassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eingabedaten = pd.DataFrame({\n",
    "    'Zellenform': variation_zellenform,\n",
    "    'Zellengroesse': variation_zellengroesse,\n",
    "    'Fuellgrad': variation_fuellgrad\n",
    "}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300e862",
   "metadata": {},
   "source": [
    "### Schritt 2: Berechnung der Prognosen\n",
    "\n",
    "Die Prognosen des ursprünglichen KI-Modells lassen sich einfach mit der\n",
    "`predict()`-Methode berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "ausgabedaten = ki_modell.predict(eingabedaten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6869d9",
   "metadata": {},
   "source": [
    "### Schritt 3: Gewichtung der Eingabedaten\n",
    "\n",
    "Für das LIME-Verfahren ist es wichtig, dass die variierte Eingabedaten gewichtet\n",
    "werden. Je ähnlicher ein Datenpunkt zum ausgewählten Beispiel ist, desto mehr\n",
    "Gewicht soll dieser Datenpunkt beim Training des Ersatzmodells haben. In diesem\n",
    "Beispiel verwenden wir den euklidischen Abstand, um die Ähnlichkeit der\n",
    "Eingabedaten zur Referenz zu berechnen.\n",
    "\n",
    "```{figure} pics/variierte_eingabedaten_annotated.svg\n",
    ":alt: Euklidischer Abstand zur Referenz\n",
    ":align: center\n",
    ":name: variierte_eingabedaten_annotate\n",
    "Der euklidische Abstand $r$ zur Referenz kann mit dem Satz des Pythagoras \n",
    "als $r=\\sqrt{(\\Delta x)^2 + (\\Delta y)^2}$ berechnet werden.\n",
    "(Quelle: eigene Darstellung)\n",
    "```\n",
    "\n",
    "Damit ergibt sich der folgende Python-Code zur Berechnung der Abstände."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d28c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstaende = ((eingabedaten['Zellengroesse'] - 3.0)**2 + (eingabedaten['Fuellgrad'] - 0.3)**2)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ec727",
   "metadata": {},
   "source": [
    "Wie die Gewichte nun basierend auf der Ähnlichkeit der Eingabedaten zur Referenz\n",
    "gewählt werden, wird in der Forschung intensiv diskutiert. Häufig werden\n",
    "exponentielle Gewichte gewählt. Wir nehmen hier Gewichte, die linear vom Abstand\n",
    "der Eingabedaten zur Referenz abhängen. Das Gewicht soll 1 sein, wenn der\n",
    "Abstand zum ausgewählten Beispiel 0 ist. Ein schneller Check der gestörten\n",
    "Eingabedaten zeigt, dass der maximale Abstand der Eingabedaten zum\n",
    "Referenzbeispiel kleiner als 1.5 ist. Für einen Abstand von 1.5 fordern wir ein\n",
    "Gewicht von 0. Dazwischen sollen die Gewichte linear abfallen.\n",
    "\n",
    "```{figure} pics/gewichtsfunktionen.svg\n",
    ":alt: Gewichtsfunktionen\n",
    ":align: center\n",
    ":name: gewichtsfunktionen\n",
    "\n",
    "Mögliche Gewichtsfunktionen: links eine lineare Gewichtsfunktion, die so\n",
    "parametriert wurde, dass ein Abstand $r=0$ zu einem Gewicht von Eins führt und\n",
    "ab $r=1.5$ Null ist. Die rechte exponentielle Gewichtsfunktion ist ähnlich zur\n",
    "linearen Gewichtsfunktion, bietet aber den zusätzlichen Vorteil, differenzierbar\n",
    "zu sein.\n",
    "(Quelle: eigene Darstellung)\n",
    "```\n",
    "\n",
    "Mit dem folgenden Code implementieren wir die lineare Gewichtsfunktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119dbec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gewichte = -2/3 * abstaende + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d26dbe",
   "metadata": {},
   "source": [
    "### Schritt 4: Training eines Ersatzmodells\n",
    "\n",
    "Häufig wird ein lineares Regressionsmodell oder ein Entscheidungsbaum verwendet,\n",
    "um ein lokal interpretierbares Modell zu erzeugen, dass das ursprüngliche\n",
    "KI-Modell erklärt. Wir werfen mit Hilfe eines Streudiagramms (Scatterplot) einen\n",
    "kurzen Blick auf die variierten Eingabedaten und die vom KI-Modell\n",
    "prognostizierten Ausgabedaten (maximale Kräfte). Auf der x-Achse tragen wir die\n",
    "Zellengrößen ein und auf der y-Achse die Füllgrade der variierten Bauteile.\n",
    "Durch die Farbe kennzeichen wir die prognostizierten maximalen Kräfte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter( eingabedaten, x='Zellengroesse', y='Fuellgrad', color=ausgabedaten,\n",
    "    title='Variierte Eingabedaten und dazugehörige Prognosen', \n",
    "    labels={'color': 'maximale Kraft [N]'}\n",
    ")\n",
    "fig.update_layout(xaxis_range=[1.5, 4.5], yaxis_range=[0.15,0.45])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f1657",
   "metadata": {},
   "source": [
    "Scheinbar ist vor allem der Füllgrad entscheidend für die Prognose der maximalen\n",
    "Kraft. Wir visualisieren daher die Prognosen der maximalen Kräfte abhängig vom\n",
    "Füllgrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px \n",
    "\n",
    "fig = px.scatter(eingabedaten, x='Fuellgrad', y=ausgabedaten,\n",
    "    title='Prognostizierte maximale Kraft abhängig vom Füllgrad', \n",
    "    labels={'y': 'maximale Kraft[N]'}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98512f84",
   "metadata": {},
   "source": [
    "Wir wählen als White-Box-Modell ein lineares Regressionsmodell, das lokal eine\n",
    "gut interpretierbare Erklärung liefern soll. Dabei beschränken wir uns zunächst\n",
    "auf den Füllgrad als Ursache $x$ und die maximale Kraft (in Newton) als Wirkung\n",
    "$y$. Wir suchen also Parameter $w$ (Steigung) und $b$ (y-Achsenabschnitt), so\n",
    "dass die lineare Funktion\n",
    "\n",
    "$$y = w\\cdot x + b$$\n",
    "\n",
    "möglichst gut die Punkte trifft. Wie gut die maximalen Kräfte basierend auf dem\n",
    "Füllgrad durch die Gerade angenähert werden, bewertet das sogenannte\n",
    "R²-Bestimmtheitsmaß (siehe [Wikipedia →\n",
    "Bestimmtheitsmaß](https://de.wikipedia.org/wiki/Bestimmtheitsmaß)).\n",
    "Normalerweise liegt das R²-Bestimmtheitsmaß zwischen 0 und 1, wobei ein Wert von\n",
    "Eins perfekt wäre, aber es kann auch negativ werden.\n",
    "\n",
    "```{admonition} Interaktive Bestimmung des linearen Regressionsmodells\n",
    ":class: miniexercise\n",
    "\n",
    "Probieren Sie aus, für welche Steigung $w$ und für welchen y-Achsenabschnitt $b$\n",
    "die lineare Regressionsgerade am besten die Datenpunkte annähert. Das\n",
    "R²-Bestimmtheitsmaß wird dabei im Titel angezeigt und sollte möglichst nahe 1 sein.\n",
    "\n",
    "<iframe src=\"https://gramschs.github.io/xai/_static/extra/linear_regression.html\"\n",
    "width=100% height=\"600\" frameborder=\"0\" scrolling=\"yes\"></iframe>\n",
    "```\n",
    "\n",
    "Lineare Regressionsmodelle sind nicht darauf beschränkt, nur *ein* Merkmal als\n",
    "Ursache zu betrachten. Wir können auch ein sogenanntes multiples lineares\n",
    "Regressionsmodell benutzen, bei dem die drei Merkmale Zellenform $x_0$,\n",
    "Zellengröße $x_1$ und Füllgrad $x_2$ linear kombiniert werden, um die maximale\n",
    "Kraft $y$ zu prognostizieren:\n",
    "\n",
    "$$y = w_0\\cdot x_0 + w_1\\cdot x_1 + w_2\\cdot x_2 + b.$$\n",
    "\n",
    "Bei drei Merkmalen haben wir nicht nur die Steigung (für den Füllgrad), sondern\n",
    "auch die Steigungen für die Zellenform und die Zellengröße. Üblicherweise werden\n",
    "diese Koeffizienten Gewichte (englisch weight) genannt und mit $w_0$, $w_1$ und\n",
    "$w_2$ abgekürzt.\n",
    "\n",
    "Die Bestimmung der bestmöglichen Gewichte $w_0$, $w_1$, $w_2$ und $b$ überlassen\n",
    "wir diesmal dem Modul Scikit-Learn.\n",
    "[Scikit-Learn](https://scikit-learn.org/stable/index.html) ist eine bekanntesten\n",
    "Bibliotheken für das maschinelle Lernen und beinhaltet auch lineare\n",
    "Regressionsmodelle. Die linearen Regressionsmodelle sind dabei in einem\n",
    "Untermodul namens `sklearn.linear_model` gesammelt. Daraus importieren wir das\n",
    "lineare Regressionsmodell `LinearRegression` und instanziieren es als `modell`.\n",
    "Dann trainieren wir das lineare Regressionsmodell mit den Eingabe- und\n",
    "Ausgabedaten und benutzen dafür die die `fit()`-Methode. Wir verzichten auf eine\n",
    "Skalierung der Daten, da diese in derselben Größenordnung liegen und lassen auch\n",
    "den üblichen Split in Trainings- und Testdaten weg. Stattdessen übergeben wir\n",
    "zusätzlich über das optionale Argument `sample_weights` noch die Gewichte, so\n",
    "dass die Ähnlichkeit eines Datenpunktes zur Referenz bei der Bestimmung der\n",
    "Gewichte berücksichtigt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "modell = LinearRegression()\n",
    "modell.fit(eingabedaten, ausgabedaten, sample_weight=gewichte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f95f7",
   "metadata": {},
   "source": [
    "Mithilfe der `score()`-Methode lassen wir die Qualität des Modells bestimmen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6246e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = modell.score(eingabedaten, ausgabedaten)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96ccedd",
   "metadata": {},
   "source": [
    "Ein Score von 1 wäre perfekt, ungefähr 0.89 ist sehr gut. Wir haben daher ein\n",
    "White-Box-Ersatzmodell gefunden, das hilft, das ausgewählte Beispiel zu\n",
    "interpretieren.\n",
    "\n",
    "### Schritt 5: Erklärung der Prognose\n",
    "\n",
    "Als nächstes lassen wir uns die Gewichte $w_0, w_1$ und $w_2$ und den\n",
    "y-Achsenabschnitt $b$ des linearen Regressionsmodells\n",
    "\n",
    "$$y = w_0\\cdot x_0 + w_1\\cdot x_1 + w_2\\cdot x_2 + b$$\n",
    "\n",
    "ausgeben. Diese werden von Scikit-Learn im trainierten Modell im Attribut\n",
    "`coef_` gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c05894",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modell.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc0c59",
   "metadata": {},
   "source": [
    "Dazu kommt noch der y-Achsenabschnitt $b$, der im Attribut `intercept_` gespeichert ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420afb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modell.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e90fc82",
   "metadata": {},
   "source": [
    "Insgesamt lautet das lineare Regressionsmodell also\n",
    "\n",
    "$$y = 0\\cdot x_0 -2.46\\cdot x_1 +  635.33 \\cdot x_2 - 134.79,$$\n",
    "\n",
    "wobei $y$ die maximale Kraft in Newton bezeichnet, $x_0$ die Zellenform, $x_1$\n",
    "die Zellengröße und $x_2$ den Füllgrad. Das Gewicht für die Zellenform ist $0$,\n",
    "sie spielt bei unsererm Erklärmodell keine Rolle. Wir haben die Zellenform auch\n",
    "nicht variiert. Die Zellengröße hat einen leicht negativen Effekt, denn\n",
    "$w_1\\approx -2.46$. Der deutlich wichtigere Effekt ist jedoch der Füllgrad\n",
    "($w_2\\approx 635.33$). Selbst wenn wir berücksichtigen, dass die Füllgrade aus\n",
    "dem Intervall $[0.2, 0.45]$ Faktor 10 kleiner sind als die Zellengrößen aus dem\n",
    "Intervall $[2, 8]$, ist $w_2$ um einiges gewichtiger als $w_1$ und hat einen\n",
    "positiven Effekt. Je höher der Füllgrad ist, desto höher ist die prognostizierte\n",
    "maximale Kraft. Damit können wir Ingenieurinnen und Ingenieuren Hinweise geben,\n",
    "wie die Bauteile für die Schuheinlage konstruiert werden sollten.\n",
    "\n",
    "## Kategorien der erklärbaren KI-Modelle\n",
    "\n",
    "Bei dem obigen Beispiel des Schuheinlagen-Orakels haben wir die LIME-Methode\n",
    "benutzt, um das KI-Modell zu erklären. LIME steht dabei für »Local Interpretable\n",
    "Model-agnostic Explanations«. Allein diese Begrifflichkeiten deuten schon an,\n",
    "dass es viele verschiedene Möglichkeiten gibt, KI-Modelle zu erklären.\n",
    "Beispielsweise werden Erklärkonzepte nach ihrem **Umfang** unterschieden. **Lokale\n",
    "Modelle** erklären, wie die Entscheidungslogik des KI-Modells für ein einzelnes\n",
    "Beispiel zustandekommt und was für Datenpunkte in der unmittelbaren\n",
    "Nachbarschaft prognostiziert werden würde. Dem gegenüber stehen **globale\n",
    "Modelle**, die einen Einblick in die Gesamtstruktur und Funktionsweise eines\n",
    "KI-Modells geben.\n",
    "\n",
    "Ein weiteres Unterscheidungsmerkmal von erklärbaren KI-Modellen ist die\n",
    "**Modellabhängigkeit**. Funktioniert die Methode für jedes KI-Modell, ist das\n",
    "erklärbare KI-Modell als unabhängig vom Originalmodell, so nennt man die Methode\n",
    "**modellagnostisch**. Das Gegenteil von modellagnostisch ist\n",
    "**modellspezifisch**. Bei modellspezifischen Methoden ist die Erklärmethode auf\n",
    "ein bestimmtes KI-Modell zugeschnitten. Ein typischer Vertreter dieser Kategorie\n",
    "ist die Analyse der sogenannten Feature Importance bei Random Forests.\n",
    "\n",
    "Es gibt noch einige weitere Unterscheidungsmerkmale. In diesem Kapitel gehen wir\n",
    "noch auf die **Komplexität** ein, bei der zwischen **intrinsischen** Modellen\n",
    "und **Post-hoc-Methoden** unterschieden wird. Intrinsische Modelle sind von sich\n",
    "aus interpretierbar  wie biespielsweise die lineare Regression oder\n",
    "Entscheidungsbäume. Dahingegen werden Post-hoc-Modelle nachträglich auf\n",
    "KI-Modelle angewendet, so wie wir im obigen Beispiel die Post-hoc-Methode LIME\n",
    "eingesetzt haben, um die Funktionsweise des Schuheinlagen-Orakels im Nachhinein\n",
    "zu erklären. Eine weitere sehr bekannte Post-hoc-Methode ist das **SHAP**-Verfahren.\n",
    "\n",
    "## Zusammenfassung und Ausblick\n",
    "\n",
    "In diesem Kapitel haben wir die Erklärbarkeit von KI-Modellen untersucht. Nach\n",
    "einer Einführung in die Relevanz des Themas haben wir das populäre\n",
    "Post-hoc-Verfahren LIME kennengelernt, das universell einsetzbar ist und die\n",
    "Entscheidungslogik für einzelne Referenzbeispiele lokal interpretieren kann.\n",
    "Außerdem haben wir wichtige Kategorien der Erklärbarkeit von KI-Modellen\n",
    "betrachtet. Im nächsten Kapitel würden wir uns mit dem SHAP-Verfahren\n",
    "beschäftigen."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
